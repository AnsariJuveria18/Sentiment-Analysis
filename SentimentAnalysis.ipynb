{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCnSsP2NUrjz"
      },
      "outputs": [],
      "source": [
        "import re#re stands fr rregular expression and is use for pattern matching\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
        "from nltk import pos_tag,ne_chunk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from textblob import TextBlob\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from typing import List,Dict,Union\n",
        "from sklearn.metrics import classification_report,confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqsV3HWDZijq",
        "outputId": "07485d03-2f4e-4640-dd9d-2d35546b73a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NLPTextProcessor:\n",
        "  def __init__(self):\n",
        "    self.stop_words=set(stopwords.words('english'))\n",
        "    self.stemmer=PorterStemmer()\n",
        "    self.lemmatizer=WordNetLemmatizer()\n",
        "    self.sentiment_analyzer=SentimentIntensityAnalyzer()\n",
        "    self.nlp=spacy.load('en_core_web_sm')#here are loading a pretrained model\n",
        "\n",
        "  def clean_text(self,text:str)->str:#text preprocessing\n",
        "    text=text.lower()\n",
        "    #remove urls\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text\n",
        "\n",
        "  def tokenize_words(self,text:str)->List[str]:\n",
        "    return word_tokenize(text)#[\"apple\",\"is\",\"a\",\"good\",\"company\"]\n",
        "\n",
        "  def tokenize_sentences(self,text:str)->List[str]:\n",
        "    return sent_tokenize(text)\n",
        "\n",
        "  def remove_stopwords(self,tokens:List[str])->List[str]:#[\"apple\",\"is\",\"good\",\"company\"]\n",
        "    return [word for word in tokens if word not in self.stop_words]\n",
        "\n",
        "  def perform_stemming(self,tokens:List[str])->List[str]:\n",
        "    return [self.stemmer.stem(word) for word in tokens]\n",
        "\n",
        "  def perform_lemmatization(self,tokens:List[str])->List[str]:\n",
        "    return [self.lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "  def get_pos_tags(self,tokens:List[str])->List[tuple]:\n",
        "     return pos_tag(tokens)\n",
        "\n",
        "  def get_named_entities(self,text:str)->str:#here we are doing name entity recognition using nltk\n",
        "\n",
        "    tokens=word_tokenize(text)#word tokenize\n",
        "    pos_tags=pos_tag(tokens)#part of speech tagging\n",
        "    named_entities=ne_chunk(pos_tags)#name entity recognition\n",
        "\n",
        "  def get_spacy_entities(self,text:str)->Dict[str,List[str]]:\n",
        "    doc=self.nlp(text)\n",
        "    entities={\n",
        "        \"PERSON\":[],\n",
        "        \"ORG\":[],\n",
        "        \"GPE\":[],\n",
        "        \"DATE\":[],\n",
        "        \"PRODUCT\":[],\n",
        "    }\n",
        "    for ent in doc.ents:\n",
        "      if ent.label_ in entities:\n",
        "        entities[ent.label_].append(ent.text)\n",
        "    return entities\n",
        "\n",
        "  def get_nltk_sentiment(self,text:str)->Dict[str,float]:#sentiment analysis using nltk\n",
        "      return self.sentiment_analyzer.polarity_scores(text)\n",
        "\n",
        "  def get_textblob_sentiment(self,text:str)->Dict[str,float]:\n",
        "      analysis=TextBlob(text)\n",
        "      return{\n",
        "          'polarity':analysis.sentiment.polarity,\n",
        "          'subjectivity':analysis.sentiment.subjectivity,\n",
        "\n",
        "      }\n",
        "  def process_text(self,text):#here iam defining process text function\n",
        "    cleaned_text=self.clean_text(text)\n",
        "\n",
        "    words=self.tokenize_words(cleaned_text)\n",
        "\n",
        "    words_no_stop=self.remove_stopwords(words)\n",
        "    lemmas=self.perform_lemmatization(words_no_stop)\n",
        "    pos_tags=self.get_pos_tags(lemmas)\n",
        "    sentences=self.tokenize_sentences(text)\n",
        "\n",
        "    nltk_sentiment=self.get_nltk_sentiment(text)\n",
        "    textblob_sentiment=self.get_textblob_sentiment(text)\n",
        "    spacy_entities=self.get_spacy_entities(text)\n",
        "    nltk_name_entities=self.get_named_entities(text)\n",
        "    return {\n",
        "            \"original_text\": text,\n",
        "            \"cleaned_text\": cleaned_text,\n",
        "            \"tokens\": words,\n",
        "            \"tokens_no_stopwords\": words_no_stop,\n",
        "            \"lemmas\": lemmas,\n",
        "            \"pos_tags\": pos_tags,\n",
        "            \"sentences\": sentences,\n",
        "            \"nltk_sentiment\": nltk_sentiment,\n",
        "            \"textblob_sentiment\": textblob_sentiment,\n",
        "            \"named_entities\": spacy_entities,\n",
        "            'nltk_name_entities':nltk_name_entities,\n",
        "        }\n",
        "\n",
        "  def label_sentiment(self,compound:float)->str:\n",
        "    if compound>=0.05:\n",
        "      return \"positive\"\n",
        "    elif compound<=-0.05:\n",
        "      return \"negative\"\n",
        "    else:\n",
        "      return \"neutral\"\n",
        "\n",
        "\n",
        "  def analyze_reviews(self,filepath:str)->pd.DataFrame:\n",
        "      df=pd.read_csv(filepath)\n",
        "      if 'review' not in df.columns:\n",
        "        raise ValueError(\"csv file muust contain a review column\")\n",
        "      results=[]\n",
        "      true_labels=[]\n",
        "      predicted_labels=[]\n",
        "      for _,row in df.iterrows():\n",
        "        review=str(row['review'])\n",
        "        true_label=str(row['label']).lower().strip() if 'label' in row else None\n",
        "        processed=self.process_text(review)#ye results lekr aa rha\n",
        "        compound=processed['nltk_sentiment']#yaaha polarity ke 4 scores store ho rhe +ve,-ve,neutral and compound\n",
        "        predicted_label=self.label_sentiment(compound['compound'])#yaha sirf compound score calling part me pass ho rha\n",
        "        results.append({\n",
        "            'review':review,\n",
        "            'nlkt_compound':compound,\n",
        "            'nltk_sentiment':predicted_label,\n",
        "            'entities':processed['named_entities'],\n",
        "          'textblob_polarity':processed['textblob_sentiment'],\n",
        "\n",
        "        })\n",
        "        if true_label:\n",
        "          true_labels.append(true_label)\n",
        "        predicted_labels.append(predicted_label)\n",
        "        result_df=pd.DataFrame(results)\n",
        "        print(classification_report(true_labels,predicted_labels))\n",
        "        print(confusion_matrix(true_labels,predicted_labels))\n",
        "        return result_df\n",
        "\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "   processor=NLPTextProcessor()\n",
        "   sample_text = \"\"\"\n",
        "    Natural language processing (NLP) is a subfield of linguistics, computer science,\n",
        "    and artificial intelligence concerned with the interactions between computers and human language.\n",
        "    Apple Inc. is an American multinational technology company headquartered in Cupertino, California.\n",
        "    I love NLP! It's amazing how computers can understand human language.\n",
        "    \"\"\"\n",
        "   processed_data=processor.process_text(sample_text)#here iam calling process text function\n",
        "\n",
        "   print(\"Original Text:\")\n",
        "   print(processed_data[\"original_text\"])\n",
        "   print(\"\\nCleaned Text:\")\n",
        "   print(processed_data[\"cleaned_text\"])\n",
        "   print(\"\\nLemmatized Tokens:\")\n",
        "   print(processed_data[\"lemmas\"])\n",
        "   print(\"\\nPOS Tags:\")\n",
        "   print(processed_data[\"pos_tags\"])\n",
        "   print(\"\\nSentiment Analysis (NLTK):\")\n",
        "   print(processed_data[\"nltk_sentiment\"])\n",
        "   print(\"\\nNamed Entities (SpaCy):\")\n",
        "   print(processed_data[\"named_entities\"])\n",
        "   fetched_df=processor.analyze_reviews(\"sample_reviews.csv\")\n",
        "   print(fetched_df)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "704HCT_aWZWp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69ffa67b-2324-47f2-a43a-c5730f90d4a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "\n",
            "    Natural language processing (NLP) is a subfield of linguistics, computer science,\n",
            "    and artificial intelligence concerned with the interactions between computers and human language.\n",
            "    Apple Inc. is an American multinational technology company headquartered in Cupertino, California.\n",
            "    I love NLP! It's amazing how computers can understand human language.\n",
            "    \n",
            "\n",
            "Cleaned Text:\n",
            "natural language processing nlp is a subfield of linguistics computer science and artificial intelligence concerned with the interactions between computers and human language apple inc is an american multinational technology company headquartered in cupertino california i love nlp its amazing how computers can understand human language\n",
            "\n",
            "Lemmatized Tokens:\n",
            "['natural', 'language', 'processing', 'nlp', 'subfield', 'linguistics', 'computer', 'science', 'artificial', 'intelligence', 'concerned', 'interaction', 'computer', 'human', 'language', 'apple', 'inc', 'american', 'multinational', 'technology', 'company', 'headquartered', 'cupertino', 'california', 'love', 'nlp', 'amazing', 'computer', 'understand', 'human', 'language']\n",
            "\n",
            "POS Tags:\n",
            "[('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('nlp', 'JJ'), ('subfield', 'NN'), ('linguistics', 'NNS'), ('computer', 'NN'), ('science', 'NN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('concerned', 'VBN'), ('interaction', 'NN'), ('computer', 'NN'), ('human', 'JJ'), ('language', 'NN'), ('apple', 'NN'), ('inc', 'JJ'), ('american', 'JJ'), ('multinational', 'NN'), ('technology', 'NN'), ('company', 'NN'), ('headquartered', 'VBD'), ('cupertino', 'NN'), ('california', 'NN'), ('love', 'VBP'), ('nlp', 'RB'), ('amazing', 'JJ'), ('computer', 'NN'), ('understand', 'VBP'), ('human', 'JJ'), ('language', 'NN')]\n",
            "\n",
            "Sentiment Analysis (NLTK):\n",
            "{'neg': 0.0, 'neu': 0.742, 'pos': 0.258, 'compound': 0.9312}\n",
            "\n",
            "Named Entities (SpaCy):\n",
            "{'PERSON': [], 'ORG': ['NLP', 'Apple Inc.', 'NLP'], 'GPE': ['Cupertino', 'California'], 'DATE': [], 'PRODUCT': []}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           1.00         1\n",
            "   macro avg       1.00      1.00      1.00         1\n",
            "weighted avg       1.00      1.00      1.00         1\n",
            "\n",
            "[[1]]\n",
            "                                       review  \\\n",
            "0  I love this product! It works wonderfully.   \n",
            "\n",
            "                                       nlkt_compound nltk_sentiment  \\\n",
            "0  {'neg': 0.0, 'neu': 0.323, 'pos': 0.677, 'comp...       positive   \n",
            "\n",
            "                                            entities  \\\n",
            "0  {'PERSON': [], 'ORG': [], 'GPE': [], 'DATE': [...   \n",
            "\n",
            "                           textblob_polarity  \n",
            "0  {'polarity': 0.8125, 'subjectivity': 0.8}  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwBPiS7gXAnp",
        "outputId": "7c2ba454-0725-4459-b3f9-0dd59ec0e015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "94JxfuTnc8Zm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "607f0114-08be-4475-8f54-a3a5e555c06b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEcSui6XXV-2",
        "outputId": "630689e7-2f54-4601-89d1-162a2ded43d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xS4DCDjOib6i",
        "outputId": "ee18646e-41d9-4550-dbc5-3de899d0ae58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bALpSybfjBYk",
        "outputId": "fad0202d-d89c-4097-f815-639a28a13522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulY2yBUpjHO1",
        "outputId": "1b0c330d-31c9-4c55-e61c-dd1d746879d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KTAr41xajOKl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}